{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc08e11f-70cf-4f62-af15-f86654875f1f",
   "metadata": {},
   "source": [
    "# Writing unit tests\n",
    "<img src='../images/xebia-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "This notebook will walk you through how to write unit tests with `pytest`.\n",
    "\n",
    "The first step is installing `pytest` with: \n",
    "\n",
    "```bash\n",
    "uv add --dev pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c82140-a306-4499-be39-9efd571a082c",
   "metadata": {},
   "source": [
    "There are multiple [test discovery rules](https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html#test-discovery), which is what enables pytest to identify where the test scripts are located. \n",
    "\n",
    "The TL;DR is: When no [testpaths](https://docs.pytest.org/en/7.1.x/reference/reference.html#confval-testpaths) or path arguments in the CLI command are specified, pytest goes through each folder and looks for files starting or ending with the substring `test_*` or `*_test`.\n",
    "\n",
    "In general terms, you should mimic the structure of your package in your `tests/` folder, prepending each file with `test_*.py` and each testing function or class with `test_*`, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e7b7d-c571-4997-9c04-48738298bb80",
   "metadata": {},
   "source": [
    "Your test folder structure could look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b06de8-b75a-40ee-973a-102b1859bad7",
   "metadata": {},
   "source": [
    "```plain\n",
    "├── src/​\n",
    "│  └─ animal_shelter/​\n",
    "│     ├── __init__.py​\n",
    "│     ├── data.py​\n",
    "│     └── features.py​\n",
    "└── tests/​\n",
    "│   ├── test_data.py​\n",
    "│   └── test_features.py​\n",
    "└── pyproject.toml​\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6f315-2b89-40aa-98b3-8f2b8e792eb3",
   "metadata": {},
   "source": [
    "#### <mark> Exercise:</mark> The first test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02fadf-bee9-47a6-831c-99c997d9c7ac",
   "metadata": {},
   "source": [
    "For example, to test the `convert_camel_case*()` function from `data.py`, you would create the file `tests/test_data.py` with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be53f9-0e59-436e-b0b4-92b3335432c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animal_shelter import data\n",
    "\n",
    "def test_convert_camel_case():\n",
    "    assert data.convert_camel_case(\"CamelCase\") == \"camel_case\"\n",
    "    assert data.convert_camel_case(\"CamelCASE\") == \"camel_case\"\n",
    "    assert data.convert_camel_case(\"camel-case\") != \"camel_case\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c9116-4748-4536-990c-9a17e949b2d8",
   "metadata": {},
   "source": [
    "**Try running `pytest​` from the virtual environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc64a6-cf25-49ed-9338-f93c2bf42749",
   "metadata": {},
   "source": [
    "The `assert` keyword defines a expression that expects a boolean. If the boolean is `True` the program continues, and if it's `False` it raises an exception, which `pytest` interprests as the test not passing.\n",
    "\n",
    "**Try adding extra `assert` statements.**\n",
    "For example,\n",
    "- What would you expect `conver_camel_case()` to return if the input is already in *snake_case*?\n",
    "- What if the input contains whitespace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00522567-bfcb-4a49-8d73-295c4be346ab",
   "metadata": {},
   "source": [
    "## Other assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43084f-73de-4b17-bd53-031fad4d0322",
   "metadata": {},
   "source": [
    "The `assert` keyword is very flexible and is all the syntaxt you would need to write a lot of tests, but there are some other assertion-like functions that come in handy.\n",
    "\n",
    "For example the `assert_series_equal()` from the `pandas.testing` module allows to easily compare that the values of two `Series` are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2864a-813d-405e-b1a4-f83518c7ff1f",
   "metadata": {},
   "source": [
    "#### <mark> Exercise: </mark> The second test\n",
    "\n",
    "Can you add the following test for the `check_has_name()` function from `features.py`?\n",
    "\n",
    "Think about in which file you need to add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6358b-d500-44e9-a6d5-3ed2247b7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "from animal_shelter import features\n",
    "\n",
    "def test_check_has_name():\n",
    "    s = pd.Series([\"Ivo\", \"Henk\", \"unknown\"])\n",
    "    result = features.check_has_name(s)\n",
    "    expected = pd.Series([True, True, False])\n",
    "    assert_series_equal(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32696563-80ca-48d6-9f65-b936c02a7a9a",
   "metadata": {},
   "source": [
    "#### <mark> Exercise: </mark> The third, fourth.... test\n",
    "\n",
    "Add now at least one unit test for each function in `features.py` using `assert_series_equal()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a1e8f-cc1f-4fab-adcf-a3c1598b725e",
   "metadata": {},
   "source": [
    "### Checking for exceptions\n",
    "\n",
    "Another common kind of assertion, for which you might want to test, is checking that something doesn't work when you expect it not to.\n",
    "In the simplest case, you can use an inequality logical comparator with an `assert` statement (e.g. `assert x != y`).\n",
    "But sometimes, you want to check that the calling function actually throws an error.\n",
    "To check for exceptions you can use the `pytest.raises()` context manager.\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_for_exceptions():\n",
    "    with pytest.raises(EXCEPTION_TYPE):\n",
    "        function_that_errors()\n",
    "```\n",
    "\n",
    "#### <mark> Exercise:</mark> Checking for exceptions \n",
    "\n",
    "- Add a test that checks that `convert_camel_case()` throws an exception when called with a value that is not a `str`. Check what kind of exception the function raises in that case.\n",
    "- Can you also add some exception checking to some of the functions in `features.py`?\n",
    "\n",
    "When checking for exceptions is important that the checks are as precise as possible, so that your tests don't pass when *any* error occurs, but only the error you were expecting. To do so, you can write explicit assertions using the exception object that is produced. If you wonder what kind of exceptions are built into Python already, you can have a look at the official Python [documentation](https://docs.python.org/3/library/exceptions.html).\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_for_exceptions():\n",
    "    with pytest.raises(EXCEPTION_TYPE) as exception:\n",
    "        function_that_errors()\n",
    "    assert \"exception message\" in str(exception.value)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd4f7f-61d6-4aaf-abdd-4bffa43c7fb8",
   "metadata": {},
   "source": [
    "### A note on testing\n",
    "\n",
    "It can be hard to see the value of testing when doing this kind of exercises, but the practice of writing *real* tests is very different than trying to test a code base post-hoc. During the development process, you usually write multiple attempts of each function you write, and those attempts can give you insight into which tests are likely to be more valuable and catch potential issues in the future. \n",
    "\n",
    "Writing a meaningful testing suite is usually an organic process and which tests are good to check for comes from experience working on a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4367e9c-b456-4625-9d16-8119b17a3c7f",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "Fixtures allow you to define functions that setup elements required by (multiple) tests.\n",
    "\n",
    "For example, if two tests use the same input (data), you can put it into a fixture. To define fixtures, you can decorate a function with the `@pytest.fixture` decorator. When you call a testing function that accepts arguments, `pytest` will check if there are fixtures with the same argument names and automatically pass them to the testing functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0dc7a-9fbb-4633-a42a-6bc9ddf20979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_all_nums(list_of_numbers):\n",
    "    assert all(type(element) is int for element in list_of_numbers)\n",
    "\n",
    "def test_sum(list_of_numbers):\n",
    "    assert sum(list_of_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef069174-2c9e-46ba-8dbd-a8a3dbd60aca",
   "metadata": {},
   "source": [
    "The example above behaves simmilarly as having a single testing function with two `assert` expressions.\n",
    "\n",
    "However, fixtures are very flexible and their utility shines through with more complex usecases.\n",
    "\n",
    "The `@fixture` decorator accepts an argument called `scope` that determines *the lifetime of fixtures*. By default `scope=\"function\"`, which means that each function that requires a fixture gets a new copy of the output of the fixture. \n",
    "\n",
    "Changing this parameter is useful if, for example, initializing the inputs required for testing is expensive computationally (e.g. connecting to a database). We can choose to group all tests that require the same input into a `class` and set the fixture `scope=\"class\"`. In this case `list_of_numbers` will be generated only once, and passed to both tests.\n",
    "\n",
    "**Warning:** If one of the tests were to mutate `list_of_numbers`, that mutation would carry to the next tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecef281-20df-4522-98db-b6916c3856c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"class\")\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "class TestListFunctions:\n",
    "    def test_all_nums(self, list_of_numbers):\n",
    "        assert all(type(element) is int for element in list_of_numbers)\n",
    "\n",
    "    def test_sum(self, list_of_numbers):\n",
    "        assert sum(list_of_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90202a3e-e6eb-40a0-8990-6a063f11da45",
   "metadata": {},
   "source": [
    "Another common fixture scope is `\"module\"`, where the fixture object is the same for all testing functions within a module (i.e. `.py` file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af9549-94a3-4e60-b96e-db9b608a23e0",
   "metadata": {},
   "source": [
    "#### <mark> Exercise:</mark> Add a fixture\n",
    "\n",
    "On `features.py`, there are two functions that generate features from the `sex_upon_outcome` variable of the data.\n",
    "\n",
    "Can you write unit tests for them (or re-use the ones you wrote in the previous exercises) that use a fixture to provide the same *mocked* input data for both tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef77ed4-d0f2-440c-9012-94cd2ecb74bc",
   "metadata": {},
   "source": [
    "## Other fixture facts\n",
    "Once you get used to working with fixtures, they usually offer a more ergonomic and modular way of designing your testing suits than writing a lot of abstractions yourself.\n",
    "\n",
    "Let's look at another few things you can do with them:\n",
    "\n",
    "#### Fixtures can depend on fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65b7aa-6375-4e25-b6f7-bc8d78868d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture()\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "@pytest.fixture()\n",
    "def list_of_more_numbers(list_of_numbers):\n",
    "    return list_of_numbers + [4, 5]\n",
    "\n",
    "def test_sum(list_of_more__numbers):\n",
    "    assert sum(list_of_more_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742bbae-1950-4325-ab45-51955a68bfe6",
   "metadata": {},
   "source": [
    "#### Testing functions can depend on multiple fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e548f64-303d-4269-be19-0c3ca7c80645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture()\n",
    "def list_of_numbers():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "@pytest.fixture()\n",
    "def more_numbers():\n",
    "    return [4, 5]\n",
    "\n",
    "def test_sum(list_of_numbers, more_numbers):\n",
    "    assert sum(list_of_more_numbers) + sum(more_numbers) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5636f2-0ea1-40e9-a8e3-a83b3be98122",
   "metadata": {},
   "source": [
    "# Pytest with pre-commit\n",
    "\n",
    "You can add `pytest` to `pre-commit` by adding a *local* hook to your `.pre-commit-config.yaml`.\n",
    "\n",
    "```yaml\n",
    "- repo: local\n",
    "  hooks:\n",
    "    - id: pytest\n",
    "      name: pytest\n",
    "      entry: pytest\n",
    "      language: system\n",
    "      pass_filenames: false\n",
    "      always_run: true\n",
    "```\n",
    "\n",
    "However, running tests automatically before commits go through might be too intrusive, so be mindful of the trade-offs of installing this hook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
